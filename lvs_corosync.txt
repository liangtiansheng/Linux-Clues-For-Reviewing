LB集群的实现：
	硬件：
		F5 BIG-IP
		Citrix NetScaler
		A10 A10
		Array
		Redware
	软件:
		lvs
		haproxy
		nginx
		ats (apache traffic server)
		perlbal
	基于工作的协议层次划分：
		传输层：
			lvs,haproxy(mode tcp)
		应用层：
			haproxy,nginx,ats,perlbal
章文嵩研发的LVS：Linux Virtual Server(layer4四层路由)
LVS:工作在INPUT链上，进来的IPVS请求报文被修改(不动原报文，在原报文头部加一个D mac和R mac转给RS)后直接过POSTROUTING链出去到RS
根据请求报文的目标IP和PORT将其转发至后端主机集群中的某一个主机(根据Loadbalance算法)
类型：
	NAT：地址转换
	DR: 直接路由
	TUN：隧道

	NAT模型:
		1、集群节点跟director必须在同一个IP网络中；
		2、RIP通常是私有地址，仅用于各集群节点间的通信；
		3、director位于client和real server之间，并负责处理进出的所有通信；
		4、realserver必须将网关指向DIP；
		5、支持端口映射；
		6、realserver可以使用任意OS；
		7、较大规模应该场景中，director易成为系统瓶颈；
		8、请求和响应报文都要经由director转发

	DR模型: Director(VIP,DIP),RealServer(VIP,RIP)
		1、保证前端路由器将目标IP为VIP的请求报文发送给director;
			解决方案：
				静态绑定
				arp_tables
				修改RS主机内核的参数
		2、RS的RIP可以使用私有地址，但也可以使用公网地址
		3、RS跟Director必须在同一物理网络中，不一定同一网段，但距离也不可能远
		4、请求报文经由Director调度，但响应报文一定不能经由Director
		5、不支持端口映射
		6、RS可以是大多数OS
		7、RS的网关不能指向DIP
		8、Director和RS都有VIP地址，所以通过mac地址寻址
		9、RS一般数据从哪来就从哪出，所以要借用lo回环口配VIP

	TUN模型：跟DR相似，但这种模型不修改请求报文的IP首部(CIP<-->VIP)，而是重新封装报文(DIP<-->RIP),realserver跨互联网
		1、集群节点可以跨越Internet；
		2、RIP、DIP、VIP必须是公网地址；
		3、director仅负责处理入站请求，响应报文则由realserver直接发往客户端；
		4、realserver网关不能指向director；
		5、只有支持隧道功能的OS才能用于realserver；
		6、不支持端口映射；
		7、这种模型MTU大小需要手动控制，不然有的路由器是不支持切片
		8、Director和RS同样都有VIP，不然拆了第一个头部，第二个头部不能识别就转发了
	FULLNAT：director通过同时修改请求报文的目标地址和源地址进行转发
		1、VIP是公网地址：RIP和DIP是私网地址，二者无须在同一网络中
		2、RS接收到的请求报文的源地址为DIP，因此要响应给DIP
		3、请求报文和响应报文都必须经由Director
		4、支持端口映射机制
		5、RS可以使用任意OS
http: stateless无状态，淘宝购物车里面的商品如何保持
	session保持：
		session绑定：1、source ip hash 2、cookie
			对某一特定服务
			对多个共享同一组RS的服务，session实现不了
		session复制集群：通过多播的方式让每个session集群节点都有session，消耗大
		session服务器：mamcached,redis(key-value, kv store)，流服务器
scheduler method:
固定调度，静态：纯粹根据算法轮询分配不考虑Server在线的连接数
	rr: 轮叫，轮询
	wrr: Weight, 计算加权公平调度
	sh: source hash, 源地址hash，session邦定（http stateless无状态，每次登陆都有可能认证）
		sh将来自同一个IP的请求始终调度至同一RS
	dh: destination hash, 无论哪个主机对同一目标的请求发送给同一Server，提高缓存命中率

动态调度方法：算法都有一定
	lc: 最少连接
		Overhead = active*256+inactive
		谁的小，挑谁
	wlc: 加权最少连接，考虑服务器性能不一样
		(active*256+inactive)/weight
	sed: 最短期望延迟
		（active+1)*256/weight
	nq: never queue（先分发再计算，改进的sed）
	LBLC: 基于本地的最少连接
		动态的DH算法
		正向代理情形下的cache server调度
	LBLCR: 基于本地的带复制功能的最少连接


默认方法：wlc

ipvsadm：用户空间的命令行工具，用于管理集群服务
ipvs: 工作内核中netfilter INPUT钩子上，支持TCP,UDP,AH,EST,AH_EST,SCTP等诸多协议
	管理集群服务
		添加：-A -t|u|f service-address [-s scheduler]
			-t: TCP协议的集群 
			-u: UDP协议的集群
				service-address:     IP:PORT
			-f: FWM: 防火墙标记 
				service-address: Mark Number
		修改：-E
		删除：-D -t|u|f service-address

		# ipvsadm -A -t 172.16.100.1:80 -s rr

	管理集群服务中的RS
		添加：-a -t|u|f service-address -r server-address [-g|i|m] [-w weight]
			-t|u|f service-address：事先定义好的某集群服务
			-r server-address: 某RS的地址，在NAT模型中，可使用IP：PORT实现端口映射；
			[-g|i|m]: LVS类型	
				-g: DR
				-i: TUN
				-m: masquerade, NAT
			[-w weight]: 定义服务器权重
		修改：-e
		删除：-d -t|u|f service-address -r server-address

		# ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.8 -m 
		# ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.9 -m
	查看
		-L|l
			-n: 数字格式显示主机地址和端口
			--stats：统计数据
			--rate: 速率
			--timeout: 显示tcp、tcpfin和udp的会话超时时长
			-c: 显示当前的ipvs连接状况
			--sort
			--daemon
	删除所有集群服务
		-C：清空ipvs规则
	保存规则
		-S 
		# ipvsadm -S > /path/to/somefile
	载入此前的规则：
		-R
		# ipvsadm -R < /path/form/somefile


各节点之间的时间偏差不应该超出1秒钟；
时间服务器来同步：
NTP：Network Time Protocol


DR: 一般arp_ignore=1 arp_announce=2
	
	VIP: MAC(DVIP)绑定到Directory的VIP上，RS的VIP就没法响应，但路由设备要是自己的
	arptables：基于mac地址作访问控制，控制RS的VIP不响应Client，只能让Director的VIP响应，就不会冲突
	kernel parameter: 2.6内核后引入更高级的内核参数，现在常用的方式
		arp_ignore: 定义接收到ARP请求时的响应级别；
			0：只要本地配有地址，就给予响应；
			1：请求的地址就在进来的接口上时，才给予响应；

		arp_announce：定义将自己地址向外通告时的通告级别；
			0：有多少接口有多少地址都向外通告；
			1：试图仅向目标网络通告与其网络匹配的地址；
			2：仅向与本地接口地址匹配的网络进行通告；

DR简单实验：DR模型中一定要配lo:0回环口的路由，不然内核不能用lo:0上的vip进行封装报文
***注意即使配了arp_announce、arp_ignore，一样要将lo:0配成32位掩码，抑制arp feedback，而且前面两个内核参数要提前改，因为先配了ip就会广播
Director:							
eth0,DIP:172.16.100.2				
eth0:0,VIP:172.16.100.1				
	#route add -host 172.16.100.1 dev eth0:0
	#curl http://172.16.100.7
	#ipvsadm -C
	#ipvsadm -A -t 172.16.100.1:80 -s wlc
	#ipvsadm -a -t 172.16.100.1:80 -r 172.16.100.7 -g -w 2
	#ipvsadm -a -t 172.16.100.1:80 -r 172.16.100.8 -g -w 1
	#ipvsadm -L -n
	#curl http://172.16.100.1(显示两次第一个网页，再显示一次第二个网页）
RS1:				
eth0,RIP1:172.16.100.7				
lo:0,VIP:172.16.100.1				
	#sysctl -w net.ipv4.conf.eth0.arp_announce=2			
	#sysctl -w net.ipv4.conf.all.arp_announce=2			
	#echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
	#echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
	#ifconfig lo:0 172.16.100.1 netmask 255.255.255.255 broadcast 172.16.100.1 up
	#route add -host 172.16.100.1 dev lo:0
		地址是内核的，请求内核的地址从哪个接口进来默认从哪个接口出去
RS2:
eth0,RIP1:172.16.100.8
lo:0,VIP:172.16.100.1
	#sysctl -w net.ipv4.conf.eth0.arp_announce=2			
	#sysctl -w net.ipv4.conf.all.arp_announce=2			 
	#echo 1 > /proc/sys/net/ipv4/conf/eth0/arp_ignore
	#echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
	#ifconfig lo:0 172.16.100.1 netmask 255.255.255.255 broadcast 172.16.100.1 up
	#route add -host 172.16.100.1 dev lo:0

DR类型中，Director和RealServer的配置脚本示例：

Director脚本:
#!/bin/bash
#
# LVS script for VS/DR
# chkconfig: - 90 10
#
. /etc/rc.d/init.d/functions
#
VIP=172.16.100.1
DIP=172.16.100.2
RIP1=172.16.100.7
RIP2=172.16.100.8
PORT=80
RSWEIGHT1=2
RSWEIGHT2=5

#
case "$1" in
start)           

  /sbin/ifconfig eth0:1 $VIP broadcast $VIP netmask 255.255.255.255 up
  /sbin/route add -host $VIP dev eth0:0

# Since this is the Director we must be able to forward packets
  echo 1 > /proc/sys/net/ipv4/ip_forward

# Clear all iptables rules.
  /sbin/iptables -F

# Reset iptables counters.
  /sbin/iptables -Z

# Clear all ipvsadm rules/services.
  /sbin/ipvsadm -C

# Add an IP virtual service for VIP 192.168.0.219 port 80
# In this recipe, we will use the round-robin scheduling method. 
# In production, however, you should use a weighted, dynamic scheduling method. 
  /sbin/ipvsadm -A -t $VIP:80 -s wlc

# Now direct packets for this VIP to
# the real server IP (RIP) inside the cluster
  /sbin/ipvsadm -a -t $VIP:80 -r $RIP1 -g -w $RSWEIGHT1
  /sbin/ipvsadm -a -t $VIP:80 -r $RIP2 -g -w $RSWEIGHT2

  /bin/touch /var/lock/subsys/ipvsadm &> /dev/null
;; 

stop)
# Stop forwarding packets
  echo 0 > /proc/sys/net/ipv4/ip_forward

# Reset ipvsadm
  /sbin/ipvsadm -C

# Bring down the VIP interface
  /sbin/ifconfig eth0:0 down
  /sbin/route del $VIP
  
  /bin/rm -f /var/lock/subsys/ipvsadm
  
  echo "ipvs is stopped..."
;;

status)
  if [ ! -e /var/lock/subsys/ipvsadm ]; then
    echo "ipvsadm is stopped ..."
  else
    echo "ipvs is running ..."
    ipvsadm -L -n
  fi
;;
*)
  echo "Usage: $0 {start|stop|status}"
;;
esac


RealServer脚本:

#!/bin/bash
#
# Script to start LVS DR real server.
# chkconfig: - 90 10
# description: LVS DR real server
#
.  /etc/rc.d/init.d/functions

VIP=172.16.100.1

host=`/bin/hostname`

case "$1" in
start)
       # Start LVS-DR real server on this machine.
        /sbin/ifconfig lo down
        /sbin/ifconfig lo up
        echo 1 > /proc/sys/net/ipv4/conf/lo/arp_ignore
        echo 2 > /proc/sys/net/ipv4/conf/lo/arp_announce
        echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
        echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce

        /sbin/ifconfig lo:0 $VIP broadcast $VIP netmask 255.255.255.255 up
        /sbin/route add -host $VIP dev lo:0

;;
stop)

        # Stop LVS-DR real server loopback device(s).
        /sbin/ifconfig lo:0 down
        echo 0 > /proc/sys/net/ipv4/conf/lo/arp_ignore
        echo 0 > /proc/sys/net/ipv4/conf/lo/arp_announce
        echo 0 > /proc/sys/net/ipv4/conf/all/arp_ignore
        echo 0 > /proc/sys/net/ipv4/conf/all/arp_announce

;;
status)

        # Status of LVS-DR real server.
        islothere=`/sbin/ifconfig lo:0 | grep $VIP`
        isrothere=`netstat -rn | grep "lo:0" | grep $VIP`
        if [ ! "$islothere" -o ! "isrothere" ];then
            # Either the route or the lo:0 device
            # not found.
            echo "LVS-DR real server Stopped."
        else
            echo "LVS-DR real server Running."
        fi
;;
*)
            # Invalid entry.
            echo "$0: Usage: $0 {start|status|stop}"
            exit 1
;;
esac



curl命令选项：
	--cacert <file> CA证书 (SSL)
	--capath <directory> CA目录 (made using c_rehash) to verify peer against (SSL)
	--compressed 要求返回是压缩的形势 (using deflate or gzip)
	--connect-timeout <seconds> 设置最大请求时间
	-H/--header <line>自定义头信息传递给服务器
	-i/--include 输出时包括protocol头信息
	-I/--head 只显示文档信息
	--interface <interface> 使用指定网络接口/地址
	-s/--silent静音模式。不输出任何东西
	-u/--user <user[:password]>设置服务器的用户和密码
	-p/--proxytunnel 使用HTTP代理


RS健康状态检查脚本示例第一版：
#!/bin/bash
#
VIP=192.168.10.3
CPORT=80
FAIL_BACK=127.0.0.1
FBSTATUS=0
RS=("192.168.10.7" "192.168.10.8")
RSTATUS=("1" "1")
RW=("2" "1")
RPORT=80
TYPE=g

add() {
  ipvsadm -a -t $VIP:$CPORT -r $1:$RPORT -$TYPE -w $2
  [ $? -eq 0 ] && return 0 || return 1
}

del() {
  ipvsadm -d -t $VIP:$CPORT -r $1:$RPORT
  [ $? -eq 0 ] && return 0 || return 1
}

while :; do
  let COUNT=0
  for I in ${RS[*]}; do
    if curl --connect-timeout 1 http://$I &> /dev/null; then
      if [ ${RSTATUS[$COUNT]} -eq 0 ]; then
         add $I ${RW[$COUNT]}
         [ $? -eq 0 ] && RSTATUS[$COUNT]=1
      fi
    else
      if [ ${RSTATUS[$COUNT]} -eq 1 ]; then
         del $I
         [ $? -eq 0 ] && RSTATUS[$COUNT]=0
      fi
    fi
    let COUNT++
  done
  sleep 5
done


RS健康状态检查脚本示例第二版：
#!/bin/bash
#
VIP=192.168.10.3
CPORT=80
FAIL_BACK=127.0.0.1
RS=("192.168.10.7" "192.168.10.8")
declare -a RSSTATUS
RW=("2" "1")
RPORT=80
TYPE=g
CHKLOOP=3
LOG=/var/log/ipvsmonitor.log

addrs() {
  ipvsadm -a -t $VIP:$CPORT -r $1:$RPORT -$TYPE -w $2
  [ $? -eq 0 ] && return 0 || return 1
}

delrs() {
  ipvsadm -d -t $VIP:$CPORT -r $1:$RPORT 
  [ $? -eq 0 ] && return 0 || return 1
}

checkrs() {
  local I=1
  while [ $I -le $CHKLOOP ]; do 
    if curl --connect-timeout 1 http://$1 &> /dev/null; then
      return 0
    fi
    let I++
  done
  return 1
}

initstatus() {
  local I
  local COUNT=0;
  for I in ${RS[*]}; do
    if ipvsadm -L -n | grep "$I:$RPORT" && > /dev/null ; then
      RSSTATUS[$COUNT]=1
    else 
      RSSTATUS[$COUNT]=0
    fi
  let COUNT++
  done
}

initstatus
while :; do
  let COUNT=0
  for I in ${RS[*]}; do
    if checkrs $I; then
      if [ ${RSSTATUS[$COUNT]} -eq 0 ]; then
         addrs $I ${RW[$COUNT]}
         [ $? -eq 0 ] && RSSTATUS[$COUNT]=1 && echo "`date +'%F %H:%M:%S'`, $I is back." >> $LOG
      fi
    else
      if [ ${RSSTATUS[$COUNT]} -eq 1 ]; then
         delrs $I
         [ $? -eq 0 ] && RSSTATUS[$COUNT]=0 && echo "`date +'%F %H:%M:%S'`, $I is gone." >> $LOG
      fi
    fi
    let COUNT++
  done 
  sleep 5
done


LVS持久连接:
	无论使用什么调度算法，LVS持久连接都能实现在一定时间内，将来自同一个客户端请求派发至此前选定的RS。

	LVS持久连接模板(内存缓冲区)：记录客户来访并实时追踪，这个模板容量决定记录量，有上限
		每一个客户端  及分配给它的RS的映射关系；
		ipvsadm -L -c
		ipvsadm -L --persistent-conn
		相对于iptables的追踪模板，性能还是不错的

	ipvsadm -A|E ... -p timeout: 加入-p就是持久连接
		timeout: 持久连接时长，默认300秒；单位是秒；
	在基于SSL，需要用到持久连接；申请证书，发布证书等等，不然会出现多次请求
	
	持久类型：
	PPC(每端口持久)：将来自于同一个客户端对同一个集群服务的请求，始终定向至此前选定的RS；     持久端口连接
		#ipvsadm -A -t 192.168.10.3:23 -s rr
		#ipvsadm -a -t 192.168.10.3:23 -r 192.168.10.7 -g -w 2
		#ipvsadm -a -t 192.168.10.3:23 -r 192.168.10.8 -g -w 2
		
		#ipvsadm -E -t 192.168.10.3:23 -s rr -p 3600
	PCC(每客户端持久)：将来自于同一个客户端对所有端口的请求，始终定向至此前选定的RS；           持久客户端连接
		把所有端口统统定义为集群服务，一律向RS转发；就算没有定义过ssh，连接Director时，也转给RS
		#ipvsadm -C
		#ipvsadm -A -t 192.168.10.3:0 -s rr -p 600
		#ipvsadm -a -t 192.168.10.3:0 -r 192.168.10.7 -g -w 2
		#ipvsadm -a -t 192.168.10.3:0 -r 192.168.10.8 -g -w 1

	PNMPP：持久防火墙标记连接
		如何只让PCC的有限或指定服务加入集群而并非所有服务，此时要用到防火墙标记
	80： RS1
	23： 同一个RS
	
	防火墙标记：在PREROUTING链上将80和23都标记为同一个数值，作为集群服务，别的服务就不参与集群，持久连接可以再接着定义
		PREROUTING	
			80: 8
			23: 8
		#ipvsadm -C
		#service ipvsadm save
		#service ipvsadm restart
		#iptable -t mangle -A PREROUTING -d 192.168.10.3 -i eth0 -p tcp --dport 80 -j MARK --set-mark 8
		#iptable -t mangle -A PREROUTING -d 192.168.10.3 -i eth0 -p tcp --dport 23 -j MARK --set-mark 8
		#ipvsadm -A -f 8 -s rr
		#ipvsadm -a -f 8 -r 192.168.10.7 -g -w 2
		#ipvsadm -a -f 8 -r 192.168.10.8 -g -w 5

		#ipvsadm -E -f 8 -s rr -p 600
			持久连接会破坏负载均衡，但web上的session、cookie等需要持久有效
			但服务器端session共享(三种方式中的一种即集群复制)的话就不需要持久连接，但目前没有成熟
		
	实际上80一般和443一起标记才有意义：	
		80, 443
		http: 
		https: 
	

HA:衡量公式A=MTBF/(MTBF+MTTR)
	MTBF: Mean Time Between Failure
	MTTR: Mean Time To Repair

	0<A<1: 百分比
		90%, 95%, 99%
		99.9% 99.99% 99.999%(极别相当高,投入很大)
Message Layer:
	heartbeat(v1,v2,v3)
		heartbeat v3（分裂成三个小项目都相互独立）
			heartbeat, pacemaker,cluster-glue
	corosync（纯message layer）+ pacemaker 组合
		这就意味着corosync的决策与heartbeat v3相差无几
	cman
	keepalived
		一定程度上是为lvs创建的，但是上述两种对lvs的扩展也很到位
CRM：Cluster Resource Manage层是附着在Message Layer层之上的
	heartbeat自带的资源管理器
		v1: haresources
		v2: 有两个资源管理器
		    haresources（兼容v1）
		    crm（就叫crm，更进的版本，受欢迎）
	pacemaker(heartbeat v3, corosync)
		v3: 资源管理器crm发展为独立的项目
	rgmanager(cman)----------------配置接口：cluster.conf, system-config-cluster, conga(webgui), cman_tool-------------
所以组合方式：
	heartbeat v1 (haresources)-----配置接口：配置文件haresources-------
	heartbeat v2 (crm)-------------配置接口：每个节点运行一个crmd守护进程，有命令行接口crmsh; GUI: hb_gui---------------
	heartbeat v3 + pacemaker-------配置接口：crmsh, pcs; GUI: hawk(suse),LCMC, pacemaker-gui----------------
	corosync + pacemaker
		corosync v1 + pacemaker(plugin)
		corosync v2 + pacemaker(standalone service)
	cman(投票完善) + rgmanager(原本是天造地设一对)
	corosync v1 + cman(作插件) + pacemaker(比rgmanager强大)

	RHCS：red hat cluster pacemaker
		RHEL5: cman +　rgmanager + conga(ricci/luci)
		RHEL6: cman +　rgmanager + conga(ricci/luci)
			corosync + pacemaker
			corosync + cman + pacemaker
		RHEL7: corosync v2 + pacemaker(corosync终于有了投票完善机制,cman彻底抛弃)


DC(designated coordinate)：指派中心，有下面两种机制
	TE: Transaction engine 
	PE: Policy engine
crmd: 提供一个管理API(套接字接口）
	有很多GUI界面
		hb_gui命令
	也有CLI接口

RA: Resource Agent
	在每个节点上都有这些脚本，接受LRM传递过来的指令对资源进行管理
RA Classes: 为资源管理器提供功能，一般接受LCM提供的参数
	Legacy heartbeat v1 RA /etc/ha.d/haresources.d/目录下的脚本
		centos7改成统一的service
	LSB (/etc/rc.d/init.d/*)支持linux bash 风格的脚本都可以做为资源代理
	OCF (Open Cluster Framework)后来比LSB更优秀的脚本风格
		pacemaker
		linbit (drbd)
	STONITH（shoot the other node in the head） 专门用来管理硬件stonith设备的
	systemd /etc/systemd/system/...

Resource Type:
	primitive: 在某个时刻只能运行于一个节点上的资源，基本资源
	clone: 把主资源克隆成n份分别放到集群中的每个节点上同时运行起来，
		匿名克隆、全局惟一克隆、状态克隆（主动、被动）
		stonith设备，一个节点出故障，所有节点必须共同作用踢除
		dlm（Distributed Lock Manager）: 分布式锁管理器
	group: 将资源归类归组，同进同退
	multi-state(master/slave): 两个节点，一个主一个从
		drbd(Distributed replicated block device)：分布式复制块设备
	资源属性：
		priority: 优先级
		target-role：started, stopped, master;
		is-managed: 是否允许集群管理此资源
		resource-stickiness：资源粘性
		allow-migrate: 是否允许迁移
	资源粘性：资源对某点的依赖程度，通过score定义
		资源是否倾向于留在当前节点
		正数：乐意
		负数：离开
		若位置约束大于资源粘性则以位置约束为准（在同一节点上二者会相加）
		node1.magedu.com: 100, 200
		node2.magedu.com: 100, inf

		IPaddr::172.16.100.1/16/eth0 httpd

	资源约束：Constraint
		排列约束: (colocation)
			资源是否能够运行于同一节点
				score:
					正值：可以在一起
					负值：不能在一起
		位置约束：(location), score(分数)
			正值：倾向于此节点
			负值：倾向于逃离于此节点
			-inf: 负无穷
			inf: 正无穷
		顺序约束: (order)多个资源启动顺序依赖关系
				vip, ipvs
					ipvs-->vip
	安装配置：
		CentOS7：corosync v2 + pacemaker
			corosync v2：有了完善的vote system
			pacemaker：独立服务
		集群全生命周期管理工具
			pcs：agent(pcsd)
			crmsh: agentless(pssh)
				crm: 两种模式
				交互式：
					配置，执行commit命令以后才生效
					crm
					configure
					property stonith-enable
				批处理：
					立即生效
		配置集群前提：
			1、时间同步
			2、基于当前正在使用的主机名互相访问
			3、是否会用到仲裁设备
		资源：
			web service:
				vip: 192.168.154.111
				httpd
				这两个资源在一个node上
	Heartbeat信息传递
		Unicat, udpu
		Multicast, udp
		Broadcast
		组播地址：用于标识一个IP组播域，IANA把D类地址留给组播使用：224.0.0.0-239.255.255.255
			永久组播地址：224.0.0.0-224.0.0.255
			临时组播地址：224.0.1.0-238.255.255.255
			本地组播地址：239.0.0.0-239.255.255.255

STONITH：
	split-brain: 集群节点无法有效获取其它节点的状态信息时，产生脑裂
		后果之一：抢占共享存储
vote system:
	少数服从多数：quorum
		total/2
		with quorum：拥有法定票数
		without quorum：不拥有法定票数
	两个节点(偶数个节点): 出现投标均等
		ping node 仲裁节点
		qdisk 仲裁磁盘

HA Cluster工作模型
	A/P：两节点模型active/passive;
		without-quorum-policy={stop|ignore|suicide|freeze}
	A/A：
资源隔离：
	节点级别：STONITH(shoot the other node on the head)
	资源级别：
		例如：FC SAN switch可以实现在存储资源级别拒绝某节点的访问(关闭光交换机的接口就行)
DAS:
	Direct Attached Storage
	直接接到主板总线，BUS
		文件：块级别
NAS：
	Network
	文件服务器：文件级别
SAN: 延长DAS线缆的设备
	主机-->封装scsi报文-->隧道封装光协议-->光缆远传-->主机解封-->识别块设备
	Storage Area network
	存储区域网络
		FC SAN：光报文
		IP SAN: iSCSI
SCSI: Small Computer System Interface


Stonith设备
1、Power Distribution Units (PDU)，电交换机，可以接受指令断掉对应节点电源
Power Distribution Units are an essential element in managing power capacity and functionality for critical network, server and data center equipment. They can provide remote load monitoring of connected equipment and individual outlet power control for remote power recycling.
2、Uninterruptible Power Supplies (UPS)
A stable power supply provides emergency power to connected equipment by supplying power from a separate source in the event of utility power failure.
3、Blade Power Control Devices
If you are running a cluster on a set of blades, then the power control device in the blade enclosure is the only candidate for fencing. Of course, this device must be
capable of managing single blade computers.
4、Lights-out Devices
Lights-out devices (IBM RSA, HP iLO, Dell DRAC) are becoming increasingly popular and may even become standard in off-the-shelf computers. However, they are inferior to UPS devices, because they share a power supply with their host (a cluster node). If a node stays without power, the device supposed to control it would be just as useless. In that case, the CRM would continue its attempts to fence the node indefinitely while all other resource operations would wait for the fencing/STONITH operation to complete.
5、Testing Devices
Testing devices are used exclusively for testing purposes. They are usually more gentle on the hardware. Once the cluster goes into production, they must be replaced
with real fencing devices.

ssh 172.16.100.1 'reboot'
meatware

STONITH的实现：
stonithd
stonithd is a daemon which can be accessed by local processes or over the network. It accepts the commands which correspond to fencing operations: reset, power-off, and power-on. It can also check the status of the fencing device.
The stonithd daemon runs on every node in the CRM HA cluster. The stonithd instance running on the DC node receives a fencing request from the CRM. It is up to this and other stonithd programs to carry out the desired fencing operation.
STONITH Plug-ins
For every supported fencing device there is a STONITH plug-in which is capable of controlling said device. A STONITH plug-in is the interface to the fencing device.
On each node, all STONITH plug-ins reside in /usr/lib/stonith/plugins (or in /usr/lib64/stonith/plugins for 64-bit architectures). All STONITH plug-ins look the same to stonithd, but are quite different on the other side reflecting the nature of the fencing device.
Some plug-ins support more than one device. A typical example is ipmilan (or external/ipmi) which implements the IPMI protocol and can control any device which supports this protocol.

epel
heartbeat v2
heartbeat - Heartbeat subsystem for High-Availability Linux
heartbeat-devel - Heartbeat development package
heartbeat-gui - Provides a gui interface to manage heartbeat clusters
heartbeat-ldirectord - Monitor daemon for maintaining high availability resources, 为ipvs高可用提供规则自动生成及后端realserver健康状态检查的组件；
heartbeat-pils - Provides a general plugin and interface loading library
heartbeat-stonith - Provides an interface to Shoot The Other Node In The Head


http://dl.fedoraproject.org/pub/epel/5/i386/repoview/letter_h.group.html

实验一：
RHEL5.8 32bit
heartbeat v1
	ha web
	node1, node2
		节点名称：用/etc/hosts文件解析
		节点名称必须跟uname -n命令的执行结果一致
		ssh互信同信
		时间要同步
node1: 
	eth0	192.168.100.6
	node1.yuliang.com
	ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
	ssh -copy-id -i .ssh/id_rsa.pub root@192.168.100.7
	vim /etc/hosts
	192.168.100.6	node1.yuliang.com
	192.168.100.7	node2.yuliang.com
	scp /etc/hosts root@192.168.100.7:/etc/
	ntpdate 192.168.0.1
	crontab -e
	*/5 **** /sbin/ntpdata 192.168.0.1 &>/dev/null
	scp /var/spool/cron/root root@192.168.100.7:/var/spool/cron

	yum install heartbeat heartbeat-devel heartbeat-gui heartbeat-ldirectord heartbeat-pils heartbeat-stonith
	cd /etc/ha.d/
	cp /usr/share/doc/heartbeat-2.1.4/{authkeys,ha.cf,haresources} ./
	ls
	vim authkeys
	auth 1 （与下来对应就行）
	1 md5 ENCRYPTOIN_PASSWORD

	vim ha.cf
		logfacility	local0(不宜与以上日志定义同时用)
		keepalive 2 多长时间发一次心跳
		initdead 等第二个节点启动时间
		mcast eth0 225.0.0.1 694 1 0 在接口上多播
		auto_failback on 故障转移过去，恢复后是否转回
		node node1.yuliang.com 集群中所有的节点必须都列出来
		node node2.yuliang.com 一定要和uname -n结果一样
	
	vim haresources(资源管理器)
		(主节点	VIP	      RA资源代理  资源代理参数..........)
		(node1	192.168.100.6 Filesystem::/dev/sda1::/data1::ext2)
		
		这个VIP由/usr/lib/heartbeat/findif脚本定义到多个网卡中与VIP属于同一网段的对应网卡的别名上
		node1.yuliang.com IPaddr::192.168.100.1/16/eth0 httpd

		(IPaddr::VIP/MASK/INTERFACE/BROADCAST_IPADDRESS)
	ls resource.d/(都是资源代理脚本)
	ls /usr/lib/heartbeat(都是heartbeat运维相关的脚本)
	service httpd stop
	chkconfig httpd off （集群中一定不能开机自启）

	scp -p authkeys haresources ha.cf node2:/etc/ha.d/
	service heartbeat start
	tail -f /var/log/messages

	/usr/lib/heartbeat/hb_standby
	tail -f /var/log/messages

	vim /etc/ha.d/haresources
		node1.yuliang.com IPaddr::192.168.100.1/16/eth0 Filesystem::192.168.100.10:/web/htdocs::/var/www/html::nfs httpd
	scp /etc/ha.d/haresources node2:/etc/ha.d/
node2:
	eth0	192.168.100.7
	node2.yuliang.com
	ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
	ssh -copy-id -i .ssh/id_rsa.pub root@192.168.100.6
	yum install heartbeat heartbeat-devel heartbeat-gui heartbeat-ldirectord heartbeat-pils heartbeat-stonith


nfs:

实验二：
heartbeat v2，作为一个服务提供一个套接字接口，管理空间提高（如GUI界面），相当一个API
v2版本crmd为那些非ha-aware的应用程序提供调用的基础平台,一旦在ha.cf文件中定义crm respawn，那么此平台无法识别
haresources里面的定义的资源格式（不兼容），/usr/lib/heartbeat/下的haresources2cib.py脚本可以将haresources里面的资源
转换成xml格式放在/var/lib/heartbeat/crm目录里面，这样crm就可以读取
CIB：Cluster Information Base
	xml格式，语法复杂
	haresources2cid.py将harecources资源转成xml
	crm命令可以管理-->进化成pacemaker后crm命令异常强大
	
在实验一的基础上
ha.cf文件中启用crm respawn
/usr/lib/heartbeat/ha_propagate同步Node1配置文件到Node2上
启动heartbeat服务
netstat -tnlp 
mgmtd	5560

实验三：
	在实验一的基础上mysql<-->nfs
nfs:172.16.100.10
	groupadd -g 3306 mysql
	useradd -u 3306 -g mysql -s /sbin/nologin -M
	mkdir /mydata
	mount /dev/myvg/mydata /mydata
	mkdir /mydata/data
	chown -R mysql.mysql /mydata/data/
	vim /etc/exports
		/web/htdocs 172.16.0.0/255.255.0.0(ro)
		/mydata	    172.16.0.0/255.255.0.0(no_root_squash,rw)
	exportfs -arv
	
node1:172.16.100.6
	groupadd -g 3306 mysql
	useradd -g 3306 -u 3306 -s /sbin/nologin -M mysql
	mkdir /mydata
	mount 172.16.100.10:/mydata /mydata
	su - mysql(不让登)
	usermod -s /bin/bash
	su - mysql
	touch a(成功)
	usermod -s /sbin/nologin
	chown -R root:mysql /usr/local/mysql/
	cd /usr/local/mysql/
	scripts/mysql_install_db --user=mysql --datadir=/mydata/data
	cp support-files/my-large.cnf /etc/my.cnf
	vim /etc/my.cnf
		datadir=/mydata/data
		innodb_file_per_table = 1
	cp support-files/mysql.server /etc/init.d/mysqld
	chkconfig --add mysqld
	chkconfig mysqld off
	service	mysqld start
	umount /mydata

	vim /etc/ha.d/haresources
		node1.yuliang.com IPaddr::172.16.100.1/16/eth0 Filesystem::172.16.100.10:/web/htdocs::/var/www/html::nfs mysqld
	scp /etc/ha.d/haresources node2:/etc/ha.d/

node2:172.16.100.7
	groupadd -g 3306 mysql
	useradd -g 3306 -u 3306 -s /sbin/nologin -M mysql
	mkdir /mydata
	mount -t nfs 172.16.100.10:/mydata /mydata
	su - mysql(不让登)
	usermod -s /bin/bash
	su - mysql
	touch a(成功)
	usermod -s /sbin/nologin
	......
	umount /mydata

	

三个配置文件：
1、密钥文件，600权限, authkeys（两节点间加密传信）
2、heartbeat服务的配置配置ha.cf
3、资源管理配置文件haresources

haresources
主节点  RA1::parameter1::parameter2 RA2

资源粘性：
	资源是否倾向于留在当前节点
		正数：乐意
		负数：离开

		node1.magedu.com: 100, 200
		node2.magedu.com: 100, inf

		IPaddr::172.16.100.1/16/eth0

HA要求:
	1、时间同步；
	2、SSH双机互信；
	3、主机名称要与uname -n，并通过/etc/hosts解析；




原理简介

　　组播报文的目的地址使用D类IP地址， 范围是从224.0.0.0到239.255.255.255。D类地址不能出现在IP报文的源IP地址字段。单播数据传输过程中，一个数据包传输的路径是从源地址路由到目的地址，利用“逐跳”（hop-by-hop）的原理在IP网络中传输。然而在ip组播环中，数据包的目的地址不是一个，而是一组，形成组地址。所有的信息接收者都加入到一个组内，并且一旦加入之后，流向组地址的数据立即开始向接收者传输，组中的所有成员都能接收到数据包。组播组中的成员是动态的，主机可以在任何时刻加入和离开组播组。


组播组分类
　　组播组可以是永久的也可以是临时的。组播组地址中，有一部分由官方分配的，称为永久组播组。永久组播组保持不变的是它的ip地址，组中的成员构成可以发生变化。永久组播组中成员的数量都可以是任意的，甚至可以为零。那些没有保留下来供永久组播组使用的ip组播地址，可以被临时组播组利用。
　　224.0.0.0～224.0.0.255为预留的组播地址（永久组地址），地址224.0.0.0保留不做分配，其它地址供路由协议使用；
　　224.0.1.0～224.0.1.255是公用组播地址，可以用于Internet；
　　224.0.2.0～238.255.255.255为用户可用的组播地址（临时组地址），全网范围内有效；
　　239.0.0.0～239.255.255.255为本地管理组播地址，仅在特定的本地范围内有效。


常用预留组播地址
　　列表如下：
　　224.0.0.0 基准地址（保留）
　　224.0.0.1 所有主机的地址 （包括所有路由器地址）
　　224.0.0.2 所有组播路由器的地址
　　224.0.0.3 不分配
　　224.0.0.4 dvmrp 路由器
　　224.0.0.5 ospf 路由器
　　224.0.0.6 ospf dr
　　224.0.0.7 st 路由器
　　224.0.0.8 st 主机
　　224.0.0.9 rip-2 路由器
　　224.0.0.10 Eigrp 路由器
　　224.0.0.11 活动代理
　　224.0.0.12 dhcp 服务器/中继代理
　　224.0.0.13 所有pim 路由器
　　224.0.0.14 rsvp 封装
　　224.0.0.15 所有cbt 路由器
　　224.0.0.16 指定sbm
　　224.0.0.17 所有sbms
　　224.0.0.18 vrrp
　　以太网传输单播ip报文的时候，目的mac地址使用的是接收者的mac地址。但是在传输组播报文时，传输目的不再是一个具体的接收者，而是一个成员不确定的组，所以使用的是组播mac地址。组播mac地址是和组播ip地址对应的。iana（internet assigned number authority）规定，组播mac地址的高24bit为0x01005e，mac 地址的低23bit为组播ip地址的低23bit。
　　由于ip组播地址的后28位中只有23位被映射到mac地址，这样就会有32个ip组播地址映射到同一mac地址上。



作业：
某公司的站点，平均页面对象有60个，静态内容45个，动态内容15个，并发访问量峰值有4000个/秒，日常访问量为2500个/秒；经测试，公司的服务器对动态内容的响应能力为500个/秒，对静态内容的响应能力为10000个/秒；混合响应能力为700个/秒；假设对数据的访问需求使用一台MySQL即可完成响应。
公司页面主要提供的服务为Discuz!X2.5所提供的论坛程序，允许用户上传附件。公司计划重新改造升级此系统，因此，需要重新设计此应用。请给出你的设计。


corosync --> pacemaker
	SUSE Linux Enterprise Server: Hawk, WebGUI
	LCMC: Linux Cluster Management Console

	RHCS: Conga(luci/ricci)，红帽专用
		webGUI
		三层：第一层只装ricci不用装pacemaker,corosync
		      第二层装luci，用来连接第一层
		      第三层通过web连到luci上，用yum安装pacemaker,corosync到每个节点上，进行一切管理
		RHCS: 
		1、每个集群都有惟一集群名称；
		2、至少有一个fence设备；
		3、至少应该有三个节点；两个节点的场景中要使用qdisk；

	keepalived: VRRP, 2节点

	pacemaker原生制作依赖于heartbeat v3，不想依赖要自己制作
		 既然安装了heartbeat v3，没corosync也行，但是这
		 里用的就是corosync，heartbeat v3不启动即可




DRBD：分布式复制块设备
	两主机通过tcp按位对应
	drdb属于内核模块，在用户空间有对应的命令行工具

RAID 1：两磁盘在同一台主机上 
	mirror两磁盘间按位对应，所以两磁盘大小必须一样

DRBD: 主从
	primary: 可执行读、写操作
	secondary: 文件系统不能挂载

	A: primay <--> B: secondary
	主从可切换

DRBD: dual primay, 双主就必须配成高可用集群
	DLM：Distributed Lock Manager


磁盘调度器：
	优化先将同磁道完成，再到别的磁道完成
	合并读请求，合并写请求；

Procotol: 三种数据同步协义
	A: Async, 异步（数据交给自己的tcp栈即返回 ）
	B：semi sync, 半同步（数据交给对方tcp栈即返回）
	C：sync, 同步（数据一直到达对方硬盘）



DRBD Source: DRBD资源
	资源名称：可以是除了空白字符外的任意ACSII码字符；
	DRBD设备：在双方节点上，此DRBD设备的设备文件；一般为/dev/drbdN，其主设备号147
	磁盘：在双方节点上，各自提供的存储设备；
	网络配置：双方数据同步时所使用的网络属性；

drbd: 2.6.33起，整合进内核



1、所有realserver都down，如何处理？
2、自写监测脚本，完成维护模式切换？
3、如何在vrrp事务发生时，发送警告邮件给指定的管理员？

vrrp_script chk_haproxy {
    script "killall -0 haproxy"
    interval 2
        # check every 2 seconds
    weight -2
        # if failed, decrease 2 of the priority
    fall 2
        # require 2 failures for failures
    rise 1
        # require 1 sucesses for ok
}


vrrp_script chk_name {
		script ""
		inerval #
		weight #
		fall 2
		rise 1
}


    track_script {
        chk_schedown
    }


Paxos算法

在网络拥塞控制领域，我们知道有一个非常有名的算法叫做Nagle算法（Nagle algorithm），这是使用它的发明人John Nagle的名字来命名的，John Nagle在1984年首次用这个算法来尝试解决福特汽车公司的网络拥塞问题（RFC 896），该问题的具体描述是：如果我们的应用程序一次产生1个字节的数据，而这个1个字节数据又以网络数据包的形式发送到远端服务器，那么就很容易导致网络由于太多的数据包而过载。比如，当用户使用Telnet连接到远程服务器时，每一次击键操作就会产生1个字节数据，进而发送出去一个数据包，所以，在典型情况下，传送一个只拥有1个字节有效数据的数据包，却要发费40个字节长包头（即ip头20字节+tcp头20字节）的额外开销，这种有效载荷（payload）利用率极其低下的情况被统称之为愚蠢窗口症候群（Silly Window Syndrome）。可以看到，这种情况对于轻负载的网络来说，可能还可以接受，但是对于重负载的网络而言，就极有可能承载不了而轻易的发生拥塞瘫痪。
针对上面提到的这个状况，Nagle算法的改进在于：如果发送端欲多次发送包含少量字符的数据包（一般情况下，后面统一称长度小于MSS的数据包为小包，与此相对，称长度等于MSS的数据包为大包，为了某些对比说明，还有中包，即长度比小包长，但又不足一个MSS的包），则发送端会先将第一个小包发送出去，而将后面到达的少量字符数据都缓存起来而不立即发送，直到收到接收端对前一个数据包报文段的ACK确认、或当前字符属于紧急数据，或者积攒到了一定数量的数据（比如缓存的字符数据已经达到数据包报文段的最大长度）等多种情况才将其组成一个较大的数据包发送出去。

TCP中的Nagle算法默认是启用的，但是它并不是适合任何情况，对于telnet或rlogin这样的远程登录应用的确比较适合（原本就是为此而设计），但是在某些应用场景下我们却又需要关闭它。 

Negale算法是指发送方发送的数据不会立即发出, 而是先放在缓冲区, 等缓存区满了再发出. 发送完一批数据后, 会等待接收方对这批数据的回应, 然后再发送下一批数据。Negale 算法适用于发送方需要发送大批量数据, 并且接收方会及时作出回应的场合, 这种算法通过减少传输数据的次数来提高通信效率。如果发送方持续地发送小批量的数据, 并且接收方不一定会立即发送响应数据, 那么Negale算法会使发送方运行很慢. 对于GUI 程序, 如网络游戏程序(服务器需要实时跟踪客户端鼠标的移动), 这个问题尤其突出. 客户端鼠标位置改动的信息需要实时发送到服务器上, 由于Negale 算法采用缓冲, 大大减低了实时响应速度, 导致客户程序运行很慢。这个时候就需要使用TCP_NODELAY选项。




